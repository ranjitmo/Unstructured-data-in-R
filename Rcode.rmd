---
title: "Playing with unstructured data in R"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Getting unstructured data into R and doing something with it

This is a tutorial on using R to load, wrangle, analyze and visualize unstructured data. There is an abundance of publicly available unstructured data on the web and the two main ways to access it are scraping it directly from websites ('webscraping') or connecting to it via a website's API.
In this part (Part 1) I go through an example of webscraping data from an aircraft accident archive website using Hadley Wickham's 'rvest' package. In Part 2 I will cover accessing twitter data via the twitter API.

### Part 1: Scraping data from a website

We want to get some insight on aircraft accidents listed on http://www.baaa-acro.com/. The information on there is displayed in a way that is not easy to download and store for further analysis so we have to use webscraping functions provided in the rvest package to connect to the site in R and extract the data we want from the underlying html document of that website.

Required packages:
```{r, message = FALSE, warning = FALSE}
library(rvest)       # provides the webscraping functions we need
library(tidyverse)   # so we can work in the tidyverse way
```

When you visit the site http://www.baaa-acro.com/ you will find that it lists all airplane accidents since 1918. We're interested in date, place, airplane model and no. fatalities for each accident but cannot get access to that information in a straight forward way as it is not available for download in, say, a csv file or other format.

This is were webscraping comes into play and we will use three functions from the rvest package to get access to the data we want and then wrangle it into a format that allows us to further analyse and visualize it:

+ read_html() *- to read the entire underlying html file of the site into an r object*

+ html_nodes() *- to select parts of the html file*

+ html_text() *- to get the raw data that is encapsulated in html tags*

Let's first read in the underlying html file of the website
```{r, message = FALSE, warning = FALSE}
y <- read_html("http://www.baaa-acro.com")
```
The R object **y** now holds an xml document that in turn captures the html code of the website. Hidden in there is the data we are after which we now have to extract.

Let's get the location of each air accident on the page. html_nodes() will help us extract the location if we pass it the correct argument. The argument we need is the name of the html node that holds the location. Let's run this first:
```{r, message = FALSE, warning = FALSE}
crashLocation <- y %>%  html_nodes("span:nth-child(5) a")

crashLocation
```

How did we know we had to pass "span:nth-child(5) a" to html_nodes() to retrieve the above nodeset that holds location?
Here is where a separate tool - http://selectorgadget.com/ - comes in handy as it is designed to help us select the exact nodes from an html document that hold the desired information. See [here](https://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html) for Hadley's tutorial on installing and using SelectorGadget. 

Finally we will use html_text() to read out the raw content that is still encapsulated by html tags in the above:
```{r, message = FALSE, warning = FALSE}
crashLocation <- y %>%  html_nodes("span:nth-child(5) a") %>% html_text()

crashLocation
```

So far we have covered a method that allows us to access desired information from an html document using read_html(), html_nodes() and html_text(), all functions from the powerful rvest package.

To speed things up let's build some code to extract data not only from that single page http://www.baaa-acro.com but loop through a larger number of its archive pages that capture info on air accidents going all the way back to 1918. At the same time we also want to extract **date**, **plane model** and **no. of fatalities** on top of **location** and combine the extracted information into a data frame.

```{r, message = FALSE, warning = FALSE}
# first we create repository lists that will hold the results of date, location, plane model and fatalities
crashDates <- list()
crashLocations <-list()
planeModels <- list()
deaths <- list()

# grid sets the page no. we want to start with up to the last page we want to access
# just two pages scraped for illustration purposes; starting with page 2 as page 1 is stored in root folder of the website (not the archives folder that we are accessing below)
grid <- c(2:3) 

# we loop through all pages set in grid and scrape date, location, plane model and fatalities into respective R objects
for ( i in seq_along( grid ) ) {
  y <- read_html( paste0("http://www.baaa-acro.com/category/archives/page/", grid[i], "/") )
  crashDate <- y %>% html_nodes(".list-crash-info span:nth-child(2)") %>% html_text()
  crashLocation <- y %>%  html_nodes("span:nth-child(5) a") %>% html_text()
  planeModel <- y %>%  html_nodes("span:nth-child(8) a") %>% html_text()
  death <- y %>%  html_nodes(".list-crash-info strong") %>% html_text() %>% as.numeric()

# the repository lists created above get filled at each iteration of the loop    
  crashDates[[i]] <- crashDate
  crashLocations[[i]] <-crashLocation
  planeModels[[i]] <- planeModel
  deaths[[i]] <- death
  }

# we build a data frame from the 4 repository lists we created 
# Note that base R 'data.frame' is used instead of tidyverse 'data_frame' as that resolves an issue with subsequent geocoding we would otherwise encounter
crash_df <- data.frame( Date = unlist(crashDates), Location = unlist(crashLocations), 
                        'Airplane model' = unlist(planeModels), Fatalities = unlist(deaths), 
                        stringsAsFactors = FALSE )
```

What does the data frame we created look like?
```{r, message = FALSE, warning = FALSE}
library(knitr)
kable(crash_df[1:5,]) # displays a data frame as a 'kable' (=knitr table)
```

To be able to display this information on airline crashes on a map, we need to geocode our variable 'Location', i.e. add longitude and latitude coordinates to each value of 'Location'. The geocode() function from David Kahle's ggmap package does just that by querying googlemaps. Note that the number of such free googlemaps queries is limitied to 2,500 a day.
```{r, message = FALSE, warning = FALSE, results = 'hide'}
library(ggmap)
crash_df <- crash_df %>% mutate_geocode(Location)
```

We can see the added columns **lon** (=longitude) and **lat** (=latitude) and with that we are ready to put air crash locations onto a map. 
```{r, message = FALSE, warning = FALSE}
kable(crash_df[1:5,])
```

With the example code above we scraped only two out of the 2,500 or so archive pages across which all the airline accident data is spread if we really wanted to gather all of it going back to 1918. We are happy with roughly the past 20 years worth of data - 1997 to 2017 - and I scraped that in a separate session and geocoded location. The geocoding sometimes produced NAs so I checked the format of **Location** in those cases and edited where necessary to fill some of the gaps for **lon** and **lat**. I wrote the edited data frame into "crash_data_last20yrs_geocoded.csv" and we can now just read that back in to proceed.

So let's visualize the locations of airline crashes via bubbles on a worldmap and let size of the bubbles represent the number of fatalities. We will use the package leaflet that provides zooming, panning and popups out of the box. We'll also use the htmltools package to format the appearance of the popups.
```{r, message = FALSE, warning = FALSE}
library(leaflet)
library(htmltools)

path <- getwd()
crash_df <- read.csv( paste0(path, "/crash_data_last20yrs_geocoded.csv"), stringsAsFactors = FALSE ) # reading in 20 years worth of air accident data (1997 to 2017)

m <- leaflet(crash_df) %>%
  addTiles() %>%  # Add default OpenStreetMap map tiles
  addCircles(lng = ~lon, lat = ~lat, stroke = FALSE, fillColor = "red", fillOpacity = 0.3,
    radius = ~Fatalities * 1000, popup = paste(                                               # we add some inline styling to format the 
      paste0("<h3 style=","color:black;", ">", crash_df$Location, "</h3>"),                   # appearance of the popups
      paste0("<h3 style=","color:black;", ">", crash_df$Fatalities, " Fatalities in ", crash_df$Date, "</h3>")
      )
  )

m  # Print the map
```

We see that the date in the popups is often too verbose, giving date as well as local time e.g. "Sep 10, 2017 at 1130 LT".
Let's split these into separate columns using "at" as the separator and just show Date in the popup.
```{r, message = FALSE, warning = FALSE}
crash_df <- separate(crash_df, Date, into = c("Date", "Local time"), sep="at")

#This is what we get
kable(crash_df[1:5,])
```

Run the map again - that's better.
```{r, message = FALSE, warning = FALSE}
m <- leaflet(crash_df) %>%
  addTiles() %>%  # Add default OpenStreetMap map tiles
  addCircles(lng = ~lon, lat = ~lat, stroke = FALSE, fillColor = "red", fillOpacity = 0.3,
    radius = ~Fatalities * 1000, popup = paste(                                               # we add some inline styling to format the 
      paste0("<h3 style=","color:black;", ">", crash_df$Location, "</h3>"),                   # appearance of the popups
      paste0("<h3 style=","color:black;", ">", crash_df$Fatalities, " Fatalities in ", crash_df$Date, "</h3>")
      )
  )
  
m  # Print the map
```

So we are done here. We have been able to visualize air crash locations across the world via bubbles where their size represents the number of fatalities. The key element was to get access to the data through webscraping and wrangle it into a format that can easily be displayed on a world map. test
